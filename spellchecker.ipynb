{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95fc6e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import textdistance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "punctuation += \"«»—…“”\"\n",
    "import unittest\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "879c2b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#словарь \"правильных\" слов (из корпуса с абстрактами из википедии - для русского) + векторное представление\n",
    "class Vocabulary:\n",
    "    def __init__(self, file = 'data/wiki_data.txt'):\n",
    "        corpus = open(file, encoding='utf8').read()\n",
    "        #словарь + количество вхождений\n",
    "        self.vocab = Counter(re.findall('\\w+', corpus.lower()))\n",
    "        \n",
    "    def vectorize(self):\n",
    "        self.word2id = list(self.vocab.keys())\n",
    "        self.id2word = {i:word for i, word in enumerate(self.vocab)}\n",
    "        self.vec = CountVectorizer(analyzer='char', max_features=10000, ngram_range=(1,3))\n",
    "        self.X = self.vec.fit_transform(self.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f39a8bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordCorrection:\n",
    "    def __init__(self, word, voc):\n",
    "        self.voc = voc\n",
    "        self.voc.vectorize()\n",
    "        self.N = sum(voc.vocab.values())\n",
    "        self.word = word.lower()\n",
    "    \n",
    "    def get_closest_match_vec(self, topn=20):\n",
    "        #сначала находим ближайших кандидатов на исправление через косинусную близость\n",
    "        v = self.voc.vec.transform([self.word])\n",
    "        similarities = cosine_distances(v, self.voc.X)[0]\n",
    "        topn = similarities.argsort()[:topn] \n",
    "        return [(self.voc.id2word[top], similarities[top]) for top in topn]\n",
    "    \n",
    "    def get_closest_match_with_metric(self, lookup,topn=20, metric=textdistance.levenshtein):\n",
    "        #среди кандидатов находим слова с минимальным расстоянием Левенштейна\n",
    "        similarities = Counter() \n",
    "        for w in lookup:\n",
    "            similarities[w] = metric.normalized_similarity(self.word, w) \n",
    "\n",
    "        return similarities.most_common(topn)\n",
    "\n",
    "    def P(self, w):\n",
    "        #вероятность слова\n",
    "        return self.voc.vocab[w] / self.N\n",
    "    \n",
    "    def predict_mistaken(self):\n",
    "        #считаем слово правильным (без опечаток), если оно есть в словаре\n",
    "        return 0 if self.word in self.vocab else 1\n",
    "    \n",
    "    def get_closest_hybrid_match(self, topn=3, metric=textdistance.damerau_levenshtein):\n",
    "        #находим близкие слова через косинусную близость\n",
    "        candidates = self.get_closest_match_vec(topn*4) \n",
    "        lookup = [cand[0] for cand in candidates]\n",
    "        #считаем расстояние Левенштейна для кандидатов \n",
    "        closest = self.get_closest_match_with_metric(lookup, metric=metric)\n",
    "        dist = closest[0][1]\n",
    "        closest_res = Counter() #словарь с кандидатами с одинаковым расстоянием редактирования (лучшим)\n",
    "        closest_res[closest[0]] = self.P(closest[0][0])\n",
    "        i = 1\n",
    "        while i < len(closest) and closest[i][1] == dist:\n",
    "            closest_res[closest[i]] = self.P(closest[i][0]) #каждому кандидату присваиваем вероятность\n",
    "            i += 1\n",
    "        self.correction = closest_res.most_common(1)[0][0]\n",
    "        return self.correction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5f2f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenize:\n",
    "    #небольшой класс для токенизации текста\n",
    "    def __init__(self, text):\n",
    "        self.text = text.lower()\n",
    "    def text2tokens(self):\n",
    "        sentences = sent_tokenize(self.text, language='russian')\n",
    "        self.tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c3cac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizeEn(Tokenize):\n",
    "    #такой же но для английского\n",
    "    def text2tokens(self):\n",
    "        sentences = sent_tokenize(self.text, language='english')\n",
    "        self.tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e04c8954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger():\n",
    "    log = []    \n",
    "    # Переопределим оператор выделения объекта.\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        # Проверим есть ли у класса этот атрибут.\n",
    "        if not hasattr(cls, '_logger'):\n",
    "            # Просим родительский класс создать объект типа, переданного в cls, то есть Logger.\n",
    "            cls._logger = super(Logger, cls).__new__(cls, *args, **kwargs)\n",
    "        return cls._logger\n",
    "        \n",
    "    def logIt(self, s: str):\n",
    "        Logger.log.append(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5dd08c",
   "metadata": {},
   "source": [
    "## фасад"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "031b404b",
   "metadata": {},
   "outputs": [],
   "source": [
    " class SpellCheck:\n",
    "    def __init__(self, text, lang = 'ru'):\n",
    "        self.lang = lang\n",
    "        if lang == 'ru':\n",
    "            self.voc = Vocabulary()\n",
    "            self.tokenize = Tokenize(text = text)\n",
    "        elif lang == 'en':\n",
    "            self.voc = Vocabulary(file = 'data/wiki_data_en.txt')\n",
    "            self.tokenize = TokenizeEn(text = text)\n",
    "        self.tokenize.text2tokens()\n",
    "        self.tokens = self.tokenize.tokenized_sentences\n",
    "        self.log = Logger()\n",
    "    \n",
    "    def text(self, text):\n",
    "        #менять текст не обновляя словарь\n",
    "        if self.lang == 'ru':\n",
    "            self.tokenize = Tokenize(text = text)\n",
    "        elif self.lang == 'en':\n",
    "            self.tokenize = TokenizeEn(text = text)\n",
    "        self.tokenize.text2tokens()\n",
    "        self.tokens = self.tokenize.tokenized_sentences\n",
    "        \n",
    "    def predict_mistaken(self, word):\n",
    "        #считаем слово правильным (без опечаток), если оно есть в словаре\n",
    "        return 0 if word in self.voc.vocab else 1\n",
    "        \n",
    "    def analyze(self):\n",
    "        self.result = ''\n",
    "        for s in self.tokens:\n",
    "            for word in s:\n",
    "                if word not in punctuation and self.predict_mistaken(word) == 1:\n",
    "                    try:\n",
    "                        corr = WordCorrection(word, self.voc)\n",
    "                        corr.get_closest_hybrid_match()\n",
    "                        word = corr.correction[0]\n",
    "                    except:\n",
    "                        #если не нашлось совсем никакого слова > оно не заменяется + записывается в логи\n",
    "                        self.log.logIt(word)\n",
    "                        word = word\n",
    "                self.result = self.result + word + ' '\n",
    "            self.result+= ' '\n",
    "        self.result = re.sub(r' ([\\.,:;!?])', r'\\1', self.result, flags=re.DOTALL)\n",
    "        self.result = self.result.rstrip()\n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8d0f65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = SpellCheck('вобще, это была страная идея')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d05cb831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'вообще, это была странная идея'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst.analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ffcec76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst.log.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7190a8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'солнце светит'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst.text('сонце светит')\n",
    "tst.analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dfe0363c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('увидел', 0.8333333333333334)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = WordCorrection('увидл', v)\n",
    "w.get_closest_hybrid_match()\n",
    "w.correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6af1115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_en = SpellCheck('Hwose wooods these are I thinc I know.', lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b4b01e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'whose woods these are i think i know.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_en.analyze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd0b32",
   "metadata": {},
   "source": [
    "## тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e972964",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestSpelling(unittest.TestCase):\n",
    "        \n",
    "    def test_sun(self):\n",
    "        a = SpellCheck('сонце светит')\n",
    "        self.assertEqual(a.analyze(), 'солнце светит')\n",
    "        \n",
    "    def test_idea(self):\n",
    "        b = SpellCheck('сонце светит')\n",
    "        b.text('вобще, это была страная идея')\n",
    "        self.assertEqual(b.analyze(), 'вообще, это была странная идея')\n",
    "        \n",
    "    def test_long_words(self):\n",
    "        c = SpellCheck('this is a loooooooong wooooooord', lang='en')\n",
    "        self.assertEqual(c.analyze(), 'this is a long word')\n",
    "        \n",
    "    def test_frost(self):\n",
    "        c = SpellCheck('Hwose wooods these are I thinc I know.', lang='en')\n",
    "        self.assertEqual(c.analyze(), 'whose woods these are i think i know.')\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f01539a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_frost (__main__.TestSpelling) ... /Users/ignatenkodarja/Library/Python/3.7/lib/python/site-packages/ipykernel_launcher.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='data/wiki_data_en.txt' mode='r' encoding='utf8'>\n",
      "  after removing the cwd from sys.path.\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "ok\n",
      "test_idea (__main__.TestSpelling) ... /Users/ignatenkodarja/Library/Python/3.7/lib/python/site-packages/ipykernel_launcher.py:4: ResourceWarning: unclosed file <_io.TextIOWrapper name='data/wiki_data.txt' mode='r' encoding='utf8'>\n",
      "  after removing the cwd from sys.path.\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "ok\n",
      "test_long_words (__main__.TestSpelling) ... FAIL\n",
      "test_sun (__main__.TestSpelling) ... ok\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_long_words (__main__.TestSpelling)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/48/0qdhk_tx72z356j9sh2k4dcm0000gn/T/ipykernel_1152/278204175.py\", line 14, in test_long_words\n",
      "    self.assertEqual(c.analyze(), 'this is a long word')\n",
      "AssertionError: 'this is a ooooooohhh ooooooohhh' != 'this is a long word'\n",
      "- this is a ooooooohhh ooooooohhh\n",
      "+ this is a long word\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 71.724s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7ff42ed24d50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec0abce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
